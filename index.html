<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Praat Web</title>
    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.9.0';
        window.transformers = { pipeline, env };
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: #f0f0f0;
            overflow: hidden;
        }

        .container {
            display: flex;
            flex-direction: column;
            height: 100vh;
        }

        .header {
            background: white;
            color: #2d3748;
            padding: 15px 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-bottom: 1px solid #e2e8f0;
        }

        .header h1 {
            font-size: 24px;
            font-weight: 500;
        }

        .toolbar {
            background: white;
            padding: 10px 30px;
            display: flex;
            gap: 10px;
            align-items: center;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .toolbar button {
            padding: 8px 16px;
            background: #f0f0f0;
            color: #2d3748;
            border: 1px solid #e2e8f0;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background 0.3s;
        }

        .toolbar button:hover {
            background: #e2e8f0;
        }

        .toolbar input[type="file"] {
            display: none;
        }

        .toolbar label {
            padding: 8px 16px;
            background: #f0f0f0;
            color: #2d3748;
            border: 1px solid #e2e8f0;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background 0.3s;
        }

        .toolbar label:hover {
            background: #e2e8f0;
        }

        .main-content {
            flex: 1;
            display: flex;
            background: #e2e8f0;
            overflow: hidden;
        }

        .sidebar {
            width: 200px;
            background: white;
            padding: 20px;
            box-shadow: 1px 0 3px rgba(0,0,0,0.1);
            overflow-y: auto;
        }

        .sidebar h3 {
            font-size: 16px;
            margin-bottom: 15px;
            color: #2d3748;
        }

        .sidebar .control-group {
            margin-bottom: 20px;
        }

        .sidebar label {
            display: block;
            font-size: 12px;
            color: #4a5568;
            margin-bottom: 5px;
        }

        .sidebar input[type="range"] {
            width: 100%;
            margin-bottom: 5px;
        }

        .sidebar select {
            width: 100%;
            padding: 5px;
            border: 1px solid #e2e8f0;
            border-radius: 4px;
            font-size: 12px;
            margin-bottom: 5px;
        }

        .sidebar .value {
            font-size: 11px;
            color: #718096;
            text-align: right;
        }

        .canvas-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            padding: 20px;
            gap: 20px;
            overflow-y: auto;
        }

        .analysis-panel {
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            padding: 15px;
        }

        .analysis-panel h4 {
            font-size: 14px;
            color: #2d3748;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .analysis-panel canvas {
            border: 1px solid #e2e8f0;
            border-radius: 4px;
            width: 100%;
            cursor: crosshair;
        }

        .status-bar {
            background: #2d3748;
            color: white;
            padding: 8px 30px;
            font-size: 12px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .status-bar .info {
            display: flex;
            gap: 20px;
        }

        .status-bar .info span {
            color: #cbd5e0;
        }

        .loading {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: rgba(0,0,0,0.8);
            color: white;
            padding: 20px 40px;
            border-radius: 8px;
            display: none;
            z-index: 1000;
        }

        .hamburger-menu {
            display: none;
            cursor: pointer;
            flex-direction: column;
            gap: 4px;
        }

        .hamburger-menu .line {
            width: 25px;
            height: 3px;
            background-color: #2d3748;
        }

        #settings-toggle {
            cursor: pointer;
            font-size: 24px;
            margin-left: 15px;
        }

        #sidebar-container {
            width: 200px;
            transition: width 0.3s;
        }

        #sidebar-container.hidden {
            width: 0;
            overflow: hidden;
        }

        @media (max-width: 768px) {
            .toolbar {
                justify-content: space-between;
                position: relative;
            }

            .toolbar-buttons {
                display: none;
                flex-direction: column;
                position: absolute;
                top: 100%;
                left: 0;
                background: white;
                width: 100%;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                padding: 10px;
                gap: 5px;
            }

            .toolbar-buttons.active {
                display: flex;
            }

            .toolbar-buttons button, .toolbar-buttons label {
                width: 100%;
                text-align: left;
            }

            .hamburger-menu {
                display: flex;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Praat Web</h1>
        </div>

        <div class="toolbar">
            <div class="hamburger-menu">
                <div class="line"></div>
                <div class="line"></div>
                <div class="line"></div>
            </div>
            <div id="settings-toggle">
                ‚öôÔ∏è
            </div>
            <div class="toolbar-buttons">
                <label for="audioFile">
                    üìÅ Load Audio
                    <input type="file" id="audioFile" accept="audio/*">
                </label>
                <button onclick="playAudio()">‚ñ∂Ô∏è Play</button>
                <button onclick="pauseAudio()">‚è∏Ô∏è Pause</button>
                <button onclick="stopAudio()">‚èπÔ∏è Stop</button>
                <button onclick="toggleRecording()" id="recordBtn">üéôÔ∏è Start Recording</button>
                <button onclick="resetView()">üîÑ Reset View</button>
                <button onclick="exportImage()">üíæ Export Image</button>
                <button onclick="startFileRecognition()" id="speechBtn">üìù File Speech Recognition</button>
                <button onclick="clearLabels()">üóëÔ∏è Clear Labels</button>
            </div>
        </div>

        <div class="main-content">
            <div id="sidebar-container" class="hidden">
                <div class="sidebar">
                    <h3>Analysis Settings</h3>

                    <div class="control-group">
                    <label>Frequency Range (Hz)</label>
                    <input type="range" id="freqMin" min="0" max="1000" value="0">
                    <div class="value">Min: <span id="freqMinValue">0</span> Hz</div>
                    <input type="range" id="freqMax" min="1000" max="8000" value="5000">
                    <div class="value">Max: <span id="freqMaxValue">5000</span> Hz</div>
                </div>

                <div class="control-group">
                    <label>Window Length (ms)</label>
                    <input type="range" id="windowLength" min="5" max="50" value="25">
                    <div class="value"><span id="windowLengthValue">25</span> ms</div>
                </div>

                <div class="control-group">
                    <label>Dynamic Range (dB)</label>
                    <input type="range" id="dynamicRange" min="20" max="100" value="50">
                    <div class="value"><span id="dynamicRangeValue">50</span> dB</div>
                </div>

                <div class="control-group">
                    <label>Pitch Range (Hz)</label>
                    <input type="range" id="pitchMin" min="50" max="200" value="80">
                    <div class="value">Min: <span id="pitchMinValue">80</span> Hz</div>
                    <input type="range" id="pitchMax" min="200" max="400" value="350">
                    <div class="value">Max: <span id="pitchMaxValue">350</span> Hz</div>
                </div>

                <div class="control-group">
                    <label>Spectrogram Type</label>
                    <select id="spectrogramType">
                        <option value="wideband">Wideband (Time Resolution Priority)</option>
                        <option value="narrowband">Narrowband (Frequency Resolution Priority)</option>
                    </select>
                </div>

                <div class="control-group">
                    <label>Color Map</label>
                    <select id="colorMap">
                        <option value="grayscale">Grayscale</option>
                        <option value="hot" selected>Hot (Blue to Red)</option>
                        <option value="viridis">Viridis</option>
                        <option value="jet">Jet</option>
                    </select>
                </div>

                <h3>Speech Recognition</h3>
                <div class="control-group">
                    <label>Recognition Language</label>
                    <select id="recognitionLanguage">
                        <option value="auto" selected>üåê Auto Detect (Whisper)</option>
                        <option value="ja-JP">Japanese (Êó•Êú¨Ë™û)</option>
                        <option value="en-US">English (US)</option>
                        <option value="en-GB">English (UK)</option>
                        <option value="zh-CN">Chinese Mandarin (‰∏≠Êñá)</option>
                        <option value="ko-KR">Korean (ÌïúÍµ≠Ïñ¥)</option>
                        <option value="es-ES">Spanish (Espa√±ol)</option>
                        <option value="fr-FR">French (Fran√ßais)</option>
                        <option value="de-DE">German (Deutsch)</option>
                        <option value="it-IT">Italian (Italiano)</option>
                        <option value="pt-BR">Brazilian Portuguese (Portugu√™s)</option>
                        <option value="ru-RU">Russian (–†—É—Å—Å–∫–∏–π)</option>
                        <option value="ar-SA">Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)</option>
                        <option value="hi-IN">Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)</option>
                        <option value="th-TH">Thai (‡πÑ‡∏ó‡∏¢)</option>
                        <option value="vi-VN">Vietnamese (Ti·∫øng Vi·ªát)</option>
                    </select>
                </div>

                <h3>Display Options</h3>
                <div class="control-group">
                    <label>
                        <input type="checkbox" id="showWaveform" checked> Waveform
                    </label>
                    <label>
                        <input type="checkbox" id="showSpectrogram" checked> Spectrogram
                    </label>
                    <label>
                        <input type="checkbox" id="showPitch" checked> Pitch
                    </label>
                    <label>
                        <input type="checkbox" id="showFormants"> Formants
                    </label>
                    <label>
                        <input type="checkbox" id="showIntensity" checked> Intensity
                    </label>
                </div>
                </div>
            </div>

            <div class="canvas-container">
                <div class="analysis-panel">
                    <h4>üåà Spectrogram & Formants</h4>
                    <canvas id="spectrogramCanvas" height="400"></canvas>
                    <p id="speechCaptions" style="margin-top: 10px; padding: 10px; background: #f7fafc; border-radius: 4px; min-height: 30px; font-size: 14px; color: #2d3748; display: none;"></p>
                </div>

                <div class="analysis-panel">
                    <h4>üìä Waveform & Intensity</h4>
                    <canvas id="waveformCanvas" height="150"></canvas>
                </div>

                <div class="analysis-panel">
                    <h4>üéµ Pitch Contour</h4>
                    <canvas id="pitchCanvas" height="150"></canvas>
                </div>
            </div>
        </div>

        <div class="status-bar">
            <div class="info">
                <span>Duration: <strong id="duration">0.000</strong> s</span>
                <span>Sample Rate: <strong id="sampleRate">0</strong> Hz</span>
                <span>Cursor: <strong id="cursorTime">0.000</strong> s</span>
                <span>Frequency: <strong id="cursorFreq">0</strong> Hz</span>
            </div>
            <div>
                <span id="statusMessage">Ready</span>
            </div>
        </div>
    </div>

    <div class="loading" id="loading">
        Analyzing audio...
    </div>

    <script>
        let audioContext = null;
        let audioBuffer = null;
        let audioSource = null;
        let isPlaying = false;
        let zoomLevel = 1;
        let panOffset = 0;

        // Speech recognition variables
        let recognition = null;
        let isRecognizing = false;
        let speechLabels = [];
        let recognitionStartTime = 0;
        let recognitionMode = null;
        let lastAudioFile = null;
        let whisperPipelinePromise = null;
        let whisperAbortRequested = false;
        let whisperTaskId = 0;
        const WHISPER_MODEL_ID = 'Xenova/whisper-small';

        // Recording variables
        let mediaRecorder = null;
        let recordedChunks = [];
        let isRecording = false;
        let recordingStartTime = 0;
        let recordingStream = null;

        const waveformCanvas = document.getElementById('waveformCanvas');
        const spectrogramCanvas = document.getElementById('spectrogramCanvas');
        const pitchCanvas = document.getElementById('pitchCanvas');

        const waveformCtx = waveformCanvas.getContext('2d');
        const spectrogramCtx = spectrogramCanvas.getContext('2d');
        const pitchCtx = pitchCanvas.getContext('2d');

        // Initialize audio context
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
        }

        // Handle file upload
        document.getElementById('audioFile').addEventListener('change', async function(e) {
            const file = e.target.files[0];
            if (file) {
                await loadAudioFile(file);
            }
        });

        async function loadAudioFile(file) {
            initAudioContext();
            lastAudioFile = file;
            document.getElementById('loading').style.display = 'block';
            document.getElementById('statusMessage').textContent = 'Loading audio...';

            try {
                const arrayBuffer = await file.arrayBuffer();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                document.getElementById('duration').textContent = audioBuffer.duration.toFixed(3);
                document.getElementById('sampleRate').textContent = audioBuffer.sampleRate;

                await analyzeAndDraw();

                document.getElementById('statusMessage').textContent = 'Audio loaded successfully';
            } catch (error) {
                console.error('Error loading audio:', error);
                document.getElementById('statusMessage').textContent = 'Error loading audio';
            } finally {
                document.getElementById('loading').style.display = 'none';
            }
        }

        async function analyzeAndDraw() {
            resizeCanvases();

            if (document.getElementById('showWaveform').checked) {
                drawWaveform();
                // Draw recording indicator on waveform
                drawRecordingIndicator(waveformCtx, waveformCanvas.width, waveformCanvas.height);
            }

            if (document.getElementById('showSpectrogram').checked) {
                await drawSpectrogram();
                // Update speech labels in paragraph element
                updateSpeechCaptions();
            }

            if (document.getElementById('showPitch').checked) {
                drawPitchContour();
            }

            if (document.getElementById('showIntensity').checked) {
                drawIntensity();
            }

            if (document.getElementById('showFormants').checked) {
                drawFormants();
            }
        }

        function resizeCanvases() {
            const container = document.querySelector('.canvas-container');
            const width = container.clientWidth - 40;

            waveformCanvas.width = width;
            spectrogramCanvas.width = width;
            pitchCanvas.width = width;
        }

        function drawWaveform() {
            if (!audioBuffer) return;

            const width = waveformCanvas.width;
            const height = waveformCanvas.height;
            const data = audioBuffer.getChannelData(0);
            const step = Math.ceil(data.length / width);
            const amplitude = height / 2;

            waveformCtx.clearRect(0, 0, width, height);

            // Draw grid
            drawGrid(waveformCtx, width, height);

            // Draw waveform
            waveformCtx.beginPath();
            waveformCtx.strokeStyle = '#2d3748';
            waveformCtx.lineWidth = 1;

            for (let i = 0; i < width; i++) {
                const index = Math.floor(i * step * zoomLevel + panOffset);
                if (index < data.length) {
                    const sample = data[index];
                    const x = i;
                    const y = (1 - sample) * amplitude;

                    if (i === 0) {
                        waveformCtx.moveTo(x, y);
                    } else {
                        waveformCtx.lineTo(x, y);
                    }
                }
            }

            waveformCtx.stroke();

            // Draw center line
            waveformCtx.beginPath();
            waveformCtx.strokeStyle = '#cbd5e0';
            waveformCtx.lineWidth = 1;
            waveformCtx.setLineDash([5, 5]);
            waveformCtx.moveTo(0, height / 2);
            waveformCtx.lineTo(width, height / 2);
            waveformCtx.stroke();
            waveformCtx.setLineDash([]);
        }

        async function drawSpectrogram() {
            if (!audioBuffer) return;

            const width = spectrogramCanvas.width;
            const height = spectrogramCanvas.height;
            const data = audioBuffer.getChannelData(0);

            spectrogramCtx.clearRect(0, 0, width, height);

            // Get settings
            const spectrogramType = document.getElementById('spectrogramType').value;
            const colorMap = document.getElementById('colorMap').value;
            const freqMax = parseInt(document.getElementById('freqMax').value);
            const dynamicRange = parseInt(document.getElementById('dynamicRange').value);

            // Set window size based on type
            const windowSize = spectrogramType === 'wideband' ? 256 : 2048;
            const hopSize = Math.floor(windowSize / 4);

            // Calculate spectrogram using FFT
            const spectrogramData = calculateSpectrogram(data, windowSize, hopSize, audioBuffer.sampleRate);

            // Draw spectrogram
            drawSpectrogramImage(spectrogramCtx, spectrogramData, width, height, freqMax, dynamicRange, colorMap);

            // Draw axes and grid
            drawSpectrogramGrid(spectrogramCtx, width, height);
            drawFrequencyScale(spectrogramCtx, width, height, freqMax);
            drawTimeScale(spectrogramCtx, width, height, audioBuffer.duration);
        }

        function calculateSpectrogram(data, windowSize, hopSize, sampleRate) {
            const numFrames = Math.floor((data.length - windowSize) / hopSize) + 1;
            const spectrogram = [];

            for (let i = 0; i < numFrames; i++) {
                const start = i * hopSize;
                const frame = data.slice(start, start + windowSize);

                // Apply Hann window
                const windowedFrame = applyHannWindow(frame);

                // Compute FFT (simplified - using DFT for demo)
                const spectrum = computeFFTMagnitude(windowedFrame);
                spectrogram.push(spectrum);
            }

            return spectrogram;
        }

        function applyHannWindow(frame) {
            const windowed = new Float32Array(frame.length);
            for (let i = 0; i < frame.length; i++) {
                const window = 0.5 - 0.5 * Math.cos(2 * Math.PI * i / (frame.length - 1));
                windowed[i] = frame[i] * window;
            }
            return windowed;
        }

        function computeFFTMagnitude(frame) {
            const N = frame.length;
            const spectrum = new Float32Array(N / 2);

            // Simple DFT implementation for demo
            for (let k = 0; k < N / 2; k++) {
                let real = 0;
                let imag = 0;

                for (let n = 0; n < N; n++) {
                    const angle = -2 * Math.PI * k * n / N;
                    real += frame[n] * Math.cos(angle);
                    imag += frame[n] * Math.sin(angle);
                }

                spectrum[k] = Math.sqrt(real * real + imag * imag);
            }

            return spectrum;
        }

        function drawSpectrogramImage(ctx, spectrogramData, width, height, freqMax, dynamicRange, colorMap) {
            if (spectrogramData.length === 0) return;

            const numFreqBins = spectrogramData[0].length;
            const numTimeFrames = spectrogramData.length;

            // Create image data
            const imageData = ctx.createImageData(width, height);
            const pixels = imageData.data;

            // Find max value for normalization
            let maxValue = 0;
            for (let i = 0; i < spectrogramData.length; i++) {
                for (let j = 0; j < spectrogramData[i].length; j++) {
                    maxValue = Math.max(maxValue, spectrogramData[i][j]);
                }
            }

            // Draw spectrogram
            for (let x = 0; x < width; x++) {
                const timeIndex = Math.floor(x * numTimeFrames / width);
                if (timeIndex >= numTimeFrames) continue;

                for (let y = 0; y < height; y++) {
                    const freqIndex = Math.floor((height - y) * numFreqBins / height);
                    if (freqIndex >= numFreqBins) continue;

                    const value = spectrogramData[timeIndex][freqIndex];
                    const normalized = value / maxValue;
                    const db = 20 * Math.log10(normalized + 1e-10);
                    const intensity = Math.max(0, Math.min(1, (db + dynamicRange) / dynamicRange));

                    const pixelIndex = (y * width + x) * 4;
                    const color = getColor(intensity, colorMap);
                    pixels[pixelIndex] = color.r;
                    pixels[pixelIndex + 1] = color.g;
                    pixels[pixelIndex + 2] = color.b;
                    pixels[pixelIndex + 3] = 255;
                }
            }

            ctx.putImageData(imageData, 0, 0);
        }

        function getColor(intensity, colorMap) {
            switch (colorMap) {
                case 'grayscale':
                    const gray = Math.floor(intensity * 255);
                    return { r: gray, g: gray, b: gray };

                case 'hot':
                    if (intensity < 0.33) {
                        return {
                            r: Math.floor(intensity * 3 * 255),
                            g: 0,
                            b: 0
                        };
                    } else if (intensity < 0.66) {
                        return {
                            r: 255,
                            g: Math.floor((intensity - 0.33) * 3 * 255),
                            b: 0
                        };
                    } else {
                        return {
                            r: 255,
                            g: 255,
                            b: Math.floor((intensity - 0.66) * 3 * 255)
                        };
                    }

                case 'viridis':
                    const v = intensity;
                    return {
                        r: Math.floor(255 * (0.267 + 0.003 * v + 1.073 * v * v - 0.908 * v * v * v)),
                        g: Math.floor(255 * (0.004 + 1.397 * v - 0.397 * v * v)),
                        b: Math.floor(255 * (0.329 + 0.359 * v + 1.529 * v * v - 2.024 * v * v * v))
                    };

                case 'jet':
                    let r, g, b;
                    if (intensity < 0.25) {
                        r = 0;
                        g = 0;
                        b = 0.5 + 2 * intensity;
                    } else if (intensity < 0.5) {
                        r = 0;
                        g = 4 * (intensity - 0.25);
                        b = 1;
                    } else if (intensity < 0.75) {
                        r = 4 * (intensity - 0.5);
                        g = 1;
                        b = 1 - 4 * (intensity - 0.5);
                    } else {
                        r = 1;
                        g = 1 - 4 * (intensity - 0.75);
                        b = 0;
                    }
                    return {
                        r: Math.floor(r * 255),
                        g: Math.floor(g * 255),
                        b: Math.floor(b * 255)
                    };

                default:
                    return { r: 0, g: 0, b: 0 };
            }
        }

        function drawSpectrogramGrid(ctx, width, height) {
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.2)';
            ctx.lineWidth = 0.5;
            ctx.setLineDash([2, 2]);

            // Horizontal lines at 1000 Hz intervals
            for (let freq = 1000; freq < 8000; freq += 1000) {
                const y = height - (freq / 8000) * height;
                ctx.beginPath();
                ctx.moveTo(0, y);
                ctx.lineTo(width, y);
                ctx.stroke();
            }

            // Vertical lines at 0.1s intervals
            for (let t = 0.1; t < 10; t += 0.1) {
                const x = (t / 10) * width;
                ctx.beginPath();
                ctx.moveTo(x, 0);
                ctx.lineTo(x, height);
                ctx.stroke();
            }

            ctx.setLineDash([]);
        }

        function drawTimeScale(ctx, width, height, duration) {
            ctx.fillStyle = '#ffffff';
            ctx.strokeStyle = '#ffffff';
            ctx.font = 'bold 11px Arial';
            ctx.textAlign = 'center';
            ctx.lineWidth = 1;

            // Draw time labels at bottom
            const numLabels = Math.min(10, Math.floor(duration) + 1);
            for (let i = 0; i <= numLabels; i++) {
                const time = (i / numLabels) * duration;
                const x = (i / numLabels) * width;

                // Draw tick
                ctx.beginPath();
                ctx.moveTo(x, height - 5);
                ctx.lineTo(x, height);
                ctx.stroke();

                // Draw label
                ctx.fillText(time.toFixed(1) + 's', x, height - 8);
            }

            // Draw axis label
            ctx.font = 'bold 12px Arial';
            ctx.fillText('Time (s)', width / 2, height - 20);
        }

        function drawPitchContour() {
            if (!audioBuffer) return;

            const width = pitchCanvas.width;
            const height = pitchCanvas.height;
            const pitchMin = parseInt(document.getElementById('pitchMin').value);
            const pitchMax = parseInt(document.getElementById('pitchMax').value);

            pitchCtx.clearRect(0, 0, width, height);

            // Draw grid
            drawGrid(pitchCtx, width, height);

            // Draw pitch contour (simulated)
            pitchCtx.beginPath();
            pitchCtx.strokeStyle = '#4a5568';
            pitchCtx.lineWidth = 2;

            for (let x = 0; x < width; x++) {
                const time = x / width;

                // Simulate pitch contour
                const pitch = pitchMin + (pitchMax - pitchMin) *
                             (Math.sin(time * 10) * 0.3 + 0.5 +
                              Math.sin(time * 30) * 0.1);

                const y = height - ((pitch - pitchMin) / (pitchMax - pitchMin)) * height;

                if (x === 0) {
                    pitchCtx.moveTo(x, y);
                } else {
                    pitchCtx.lineTo(x, y);
                }
            }

            pitchCtx.stroke();

            // Draw pitch scale
            drawPitchScale(pitchCtx, width, height, pitchMin, pitchMax);
        }

        function drawIntensity() {
            if (!audioBuffer) return;

            const width = waveformCanvas.width;
            const height = waveformCanvas.height;
            const data = audioBuffer.getChannelData(0);
            const windowSize = Math.floor(audioBuffer.sampleRate * 0.01);

            waveformCtx.strokeStyle = 'rgba(255, 152, 0, 0.7)';
            waveformCtx.lineWidth = 2;
            waveformCtx.beginPath();

            for (let x = 0; x < width; x++) {
                const startIndex = Math.floor((x / width) * data.length);
                const endIndex = Math.min(startIndex + windowSize, data.length);

                let sum = 0;
                for (let i = startIndex; i < endIndex; i++) {
                    sum += data[i] * data[i];
                }

                const rms = Math.sqrt(sum / (endIndex - startIndex));
                const db = 20 * Math.log10(rms + 0.0001);
                const normalized = Math.max(0, Math.min(1, (db + 60) / 60));
                const y = height - normalized * height;

                if (x === 0) {
                    waveformCtx.moveTo(x, y);
                } else {
                    waveformCtx.lineTo(x, y);
                }
            }

            waveformCtx.stroke();
        }

        function drawFormants() {
            if (!audioBuffer) return;

            const width = spectrogramCanvas.width;
            const height = spectrogramCanvas.height;
            const data = audioBuffer.getChannelData(0);

            // Formant colors - high contrast for visibility on spectrogram
            const formantColors = [
                { color: 'rgba(255, 255, 255, 0.9)', label: 'F1' },     // White
                { color: 'rgba(255, 255, 0, 0.9)', label: 'F2' },       // Yellow
                { color: 'rgba(0, 255, 255, 0.9)', label: 'F3' },       // Cyan
                { color: 'rgba(255, 128, 0, 0.9)', label: 'F4' },       // Orange
                { color: 'rgba(128, 255, 128, 0.9)', label: 'F5' }      // Light Green
            ];

            const freqMax = parseInt(document.getElementById('freqMax').value);

            // Estimate formants
            const formantTracks = estimateFormants(data, audioBuffer.sampleRate);

            // Draw each formant track as an ultra-smooth curve
            formantTracks.forEach((track, formantIndex) => {
                if (formantIndex >= formantColors.length || track.length < 3) return;

                const color = formantColors[formantIndex];

                // Filter points within frequency range
                const validPoints = [];
                for (let i = 0; i < track.length; i++) {
                    const point = track[i];
                    if (point && point.frequency > 0 && point.frequency < freqMax) {
                        validPoints.push({
                            x: (i / (track.length - 1)) * width,
                            y: height - (point.frequency / freqMax) * height
                        });
                    }
                }

                if (validPoints.length < 2) return;

                // Draw using cubic Bezier spline for ultra-smooth curves
                spectrogramCtx.strokeStyle = color.color;
                spectrogramCtx.lineWidth = 2.5;
                spectrogramCtx.lineCap = 'round';
                spectrogramCtx.lineJoin = 'round';
                spectrogramCtx.beginPath();

                // Use Catmull-Rom spline for smooth interpolation
                drawCatmullRomSpline(spectrogramCtx, validPoints);
                spectrogramCtx.stroke();

                // Draw label at the end
                if (validPoints.length > 0) {
                    const lastPoint = validPoints[validPoints.length - 1];

                    // Draw label background for better visibility
                    spectrogramCtx.fillStyle = 'rgba(0, 0, 0, 0.7)';
                    spectrogramCtx.fillRect(lastPoint.x + 5, lastPoint.y - 8, 20, 16);

                    // Draw label text
                    spectrogramCtx.fillStyle = color.color;
                    spectrogramCtx.font = 'bold 11px Arial';
                    spectrogramCtx.fillText(color.label, lastPoint.x + 7, lastPoint.y + 2);
                }
            });
        }

        function drawCatmullRomSpline(ctx, points) {
            if (points.length < 2) return;

            ctx.moveTo(points[0].x, points[0].y);

            // Add control points at the beginning and end
            const extendedPoints = [
                points[0], // Duplicate first point
                ...points,
                points[points.length - 1] // Duplicate last point
            ];

            // Draw Catmull-Rom spline
            for (let i = 1; i < extendedPoints.length - 2; i++) {
                const p0 = extendedPoints[i - 1];
                const p1 = extendedPoints[i];
                const p2 = extendedPoints[i + 1];
                const p3 = extendedPoints[i + 2];

                // Calculate control points
                const cp1x = p1.x + (p2.x - p0.x) / 6;
                const cp1y = p1.y + (p2.y - p0.y) / 6;
                const cp2x = p2.x - (p3.x - p1.x) / 6;
                const cp2y = p2.y - (p3.y - p1.y) / 6;

                ctx.bezierCurveTo(cp1x, cp1y, cp2x, cp2y, p2.x, p2.y);
            }
        }

        function estimateFormants(data, sampleRate) {
            // Balanced formant estimation - accurate but performant
            const windowSize = 512;
            const hopSize = 256; // Balance between resolution and performance
            const maxFrames = 150; // Limit for performance
            const totalFrames = Math.floor((data.length - windowSize) / hopSize);
            const skipFactor = Math.max(1, Math.floor(totalFrames / maxFrames));
            const numFrames = Math.min(maxFrames, totalFrames);

            // Initialize formant tracks for 5 formants
            const formantTracks = [[], [], [], [], []];

            // Typical formant ranges for adult speech
            const typicalFormants = [
                { min: 200, max: 1000, center: 700, bandwidth: 100 },   // F1
                { min: 800, max: 2500, center: 1500, bandwidth: 150 },  // F2
                { min: 1500, max: 3500, center: 2500, bandwidth: 200 }, // F3
                { min: 2500, max: 4500, center: 3500, bandwidth: 300 }, // F4
                { min: 3500, max: 5500, center: 4500, bandwidth: 400 }  // F5
            ];

            // Process frames with actual FFT
            const frameData = [];
            for (let frameIdx = 0; frameIdx < numFrames; frameIdx++) {
                const actualFrame = frameIdx * skipFactor;
                const start = actualFrame * hopSize;
                const frame = data.slice(start, start + windowSize);

                if (frame.length < windowSize) break;

                // Apply window
                const windowedFrame = applyHannWindow(frame);

                // Compute spectrum (simplified FFT)
                const spectrum = computeFFTMagnitude(windowedFrame);
                frameData.push(spectrum);
            }

            // Track formants through frames
            for (let formantIdx = 0; formantIdx < typicalFormants.length; formantIdx++) {
                const formant = typicalFormants[formantIdx];
                const track = [];
                let prevFreq = formant.center;

                for (let i = 0; i < frameData.length; i++) {
                    const spectrum = frameData[i];

                    // Find peak in formant range
                    let maxAmp = 0;
                    let peakFreq = prevFreq;

                    const minBin = Math.floor(formant.min * windowSize / sampleRate);
                    const maxBin = Math.floor(formant.max * windowSize / sampleRate);

                    // Find local maxima in the range
                    for (let bin = minBin + 1; bin < maxBin - 1 && bin < spectrum.length - 1; bin++) {
                        // Check if local maximum
                        if (spectrum[bin] > spectrum[bin - 1] && spectrum[bin] > spectrum[bin + 1]) {
                            const freq = (bin * sampleRate) / windowSize;

                            // Weight by amplitude and continuity
                            const amp = spectrum[bin];
                            const continuityWeight = Math.exp(-Math.abs(freq - prevFreq) / formant.bandwidth);
                            const weightedAmp = amp * continuityWeight;

                            if (weightedAmp > maxAmp) {
                                maxAmp = weightedAmp;
                                peakFreq = freq;
                            }
                        }
                    }

                    // Smooth transition (less aggressive smoothing to preserve data)
                    const smoothedFreq = prevFreq * 0.7 + peakFreq * 0.3;
                    prevFreq = smoothedFreq;

                    track.push({
                        frequency: smoothedFreq,
                        amplitude: maxAmp
                    });
                }

                // Apply smoothing to reduce jitter
                const smoothedTrack = applyMovingAverage(track, 5);
                formantTracks[formantIdx] = smoothedTrack;
            }

            return formantTracks;
        }

        function applyMovingAverage(track, windowSize) {
            if (track.length < windowSize) return track;

            const smoothed = [];
            const halfWindow = Math.floor(windowSize / 2);

            for (let i = 0; i < track.length; i++) {
                let freqSum = 0;
                let ampSum = 0;
                let weights = 0;

                for (let j = -halfWindow; j <= halfWindow; j++) {
                    const idx = i + j;
                    if (idx >= 0 && idx < track.length) {
                        // Gaussian weight for smoother results
                        const weight = Math.exp(-(j * j) / (halfWindow * halfWindow));
                        freqSum += track[idx].frequency * weight;
                        ampSum += track[idx].amplitude * weight;
                        weights += weight;
                    }
                }

                smoothed.push({
                    frequency: freqSum / weights,
                    amplitude: ampSum / weights
                });
            }

            return smoothed;
        }

        function smoothSpectrum(spectrum, windowSize) {
            const smoothed = new Float32Array(spectrum.length);
            const halfWindow = Math.floor(windowSize / 2);

            for (let i = 0; i < spectrum.length; i++) {
                let sum = 0;
                let count = 0;

                for (let j = Math.max(0, i - halfWindow); j <= Math.min(spectrum.length - 1, i + halfWindow); j++) {
                    sum += spectrum[j];
                    count++;
                }

                smoothed[i] = sum / count;
            }

            return smoothed;
        }

        function findSpectralPeaks(spectrum, sampleRate, windowSize) {
            const peaks = [];
            const minPeakDistance = 5; // Minimum bin distance between peaks

            for (let i = 1; i < spectrum.length - 1; i++) {
                // Check if this is a local maximum
                if (spectrum[i] > spectrum[i - 1] && spectrum[i] > spectrum[i + 1]) {
                    // Check minimum distance from previous peak
                    if (peaks.length === 0 || i - peaks[peaks.length - 1].bin >= minPeakDistance) {
                        const frequency = (i * sampleRate) / windowSize;
                        peaks.push({
                            bin: i,
                            frequency: frequency,
                            amplitude: spectrum[i]
                        });
                    }
                }
            }

            // Sort by amplitude and return top peaks
            peaks.sort((a, b) => b.amplitude - a.amplitude);
            return peaks.slice(0, 10); // Return top 10 peaks
        }

        function drawGrid(ctx, width, height) {
            ctx.strokeStyle = '#e2e8f0';
            ctx.lineWidth = 1;

            // Vertical lines (time)
            for (let i = 0; i <= 10; i++) {
                const x = (i / 10) * width;
                ctx.beginPath();
                ctx.moveTo(x, 0);
                ctx.lineTo(x, height);
                ctx.stroke();
            }

            // Horizontal lines
            for (let i = 0; i <= 5; i++) {
                const y = (i / 5) * height;
                ctx.beginPath();
                ctx.moveTo(0, y);
                ctx.lineTo(width, y);
                ctx.stroke();
            }
        }

        function drawFrequencyScale(ctx, width, height, maxFreq) {
            ctx.fillStyle = '#ffffff';
            ctx.strokeStyle = '#ffffff';
            ctx.font = 'bold 11px Arial';
            ctx.textAlign = 'right';
            ctx.lineWidth = 1;

            // Draw frequency labels on left side
            for (let i = 0; i <= 5; i++) {
                const freq = (maxFreq / 5) * (5 - i);
                const y = (i / 5) * height;

                // Draw tick
                ctx.beginPath();
                ctx.moveTo(0, y);
                ctx.lineTo(5, y);
                ctx.stroke();

                // Draw label
                ctx.fillText(freq.toFixed(0) + ' Hz', 45, y + 3);
            }

            // Draw axis label
            ctx.save();
            ctx.font = 'bold 12px Arial';
            ctx.translate(15, height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.textAlign = 'center';
            ctx.fillText('Frequency (Hz)', 0, 0);
            ctx.restore();
        }

        function drawPitchScale(ctx, width, height, minPitch, maxPitch) {
            ctx.fillStyle = '#4a5568';
            ctx.font = '10px Arial';
            ctx.textAlign = 'right';

            for (let i = 0; i <= 5; i++) {
                const pitch = minPitch + ((maxPitch - minPitch) / 5) * (5 - i);
                const y = (i / 5) * height;
                ctx.fillText(pitch.toFixed(0) + ' Hz', width - 5, y + 3);
            }
        }

        // Audio controls
        function playAudio() {
            if (!audioBuffer || isPlaying) return;

            initAudioContext();
            audioSource = audioContext.createBufferSource();
            audioSource.buffer = audioBuffer;
            audioSource.connect(audioContext.destination);
            audioSource.start(0);
            isPlaying = true;

            audioSource.onended = () => {
                isPlaying = false;
            };

            document.getElementById('statusMessage').textContent = 'Playing...';
        }

        function pauseAudio() {
            if (audioContext && audioContext.state === 'running') {
                audioContext.suspend();
                document.getElementById('statusMessage').textContent = 'Paused';
            } else if (audioContext && audioContext.state === 'suspended') {
                audioContext.resume();
                document.getElementById('statusMessage').textContent = 'Playing...';
            }
        }

        function stopAudio() {
            if (audioSource && isPlaying) {
                audioSource.stop();
                isPlaying = false;
                document.getElementById('statusMessage').textContent = 'Stopped';
            }
        }

        function zoomIn() {
            zoomLevel = Math.max(0.1, zoomLevel * 0.8);
            if (audioBuffer) {
                analyzeAndDraw();
            }
        }

        function zoomOut() {
            zoomLevel = Math.min(10, zoomLevel * 1.2);
            if (audioBuffer) {
                analyzeAndDraw();
            }
        }

        function resetView() {
            zoomLevel = 1;
            panOffset = 0;
            if (audioBuffer) {
                analyzeAndDraw();
            }
        }

        function exportImage() {
            const tempCanvas = document.createElement('canvas');
            const tempCtx = tempCanvas.getContext('2d');

            // Calculate height including space for captions if they exist
            let captionHeight = 0;
            if (speechLabels.length > 0) {
                // Estimate height needed for captions with more compact layout
                captionHeight = Math.min(100, 20 + speechLabels.length * 18);
            }

            tempCanvas.width = spectrogramCanvas.width;
            tempCanvas.height = waveformCanvas.height + spectrogramCanvas.height + pitchCanvas.height + 80 + captionHeight;

            // White background
            tempCtx.fillStyle = 'white';
            tempCtx.fillRect(0, 0, tempCanvas.width, tempCanvas.height);

            // Draw title
            tempCtx.fillStyle = '#2d3748';
            tempCtx.font = 'bold 16px Arial';
            tempCtx.fillText('Praat Analysis Visualization', 10, 20);

            let yOffset = 40;

            // Add speech captions if they exist (right after title with spacing)
            if (speechLabels.length > 0) {
                // Add one line spacing after title
                yOffset += 20;

                // Sort labels by timestamp
                const sortedLabels = speechLabels.slice().sort((a, b) => a.timestamp - b.timestamp);

                // Combine all text into one line with timestamps
                const captionTexts = sortedLabels.map(label => {
                    const timeStr = label.timestamp ? `[${label.timestamp.toFixed(1)}s]` : '';
                    return `${timeStr} ${label.text}`;
                });
                const fullText = captionTexts.join(' ');

                // Draw caption text
                tempCtx.fillStyle = '#2d3748';
                tempCtx.font = '14px Arial';

                // Word wrap the combined text
                const maxWidth = tempCanvas.width - 20;
                const words = fullText.split(' ');
                let line = '';
                let textY = yOffset;

                for (let n = 0; n < words.length; n++) {
                    const testLine = line + words[n] + ' ';
                    const metrics = tempCtx.measureText(testLine);
                    const testWidth = metrics.width;

                    if (testWidth > maxWidth && n > 0) {
                        tempCtx.fillText(line.trim(), 10, textY);
                        line = words[n] + ' ';
                        textY += 18;
                    } else {
                        line = testLine;
                    }
                }
                if (line.trim()) {
                    tempCtx.fillText(line.trim(), 10, textY);
                    textY += 18;
                }

                // Add one line spacing after captions
                yOffset = textY + 20;
            }

            // Copy canvases
            tempCtx.drawImage(spectrogramCanvas, 0, yOffset);
            yOffset += spectrogramCanvas.height + 10;
            tempCtx.drawImage(waveformCanvas, 0, yOffset);
            yOffset += waveformCanvas.height + 10;
            tempCtx.drawImage(pitchCanvas, 0, yOffset);

            // Export
            const link = document.createElement('a');
            link.download = 'praat_analysis.png';
            link.href = tempCanvas.toDataURL();
            link.click();

            document.getElementById('statusMessage').textContent = 'Image exported';
        }

        // Handle mouse events for cursor tracking
        [waveformCanvas, spectrogramCanvas, pitchCanvas].forEach(canvas => {
            canvas.addEventListener('mousemove', function(e) {
                const rect = canvas.getBoundingClientRect();
                const x = e.clientX - rect.left;
                const time = (x / canvas.width) * (audioBuffer ? audioBuffer.duration : 1);

                document.getElementById('cursorTime').textContent = time.toFixed(3);

                if (canvas === spectrogramCanvas) {
                    const y = e.clientY - rect.top;
                    const freqMax = parseInt(document.getElementById('freqMax').value);
                    const freq = (1 - y / canvas.height) * freqMax;
                    document.getElementById('cursorFreq').textContent = freq.toFixed(0);
                }
            });
        });

        // Handle control changes
        ['freqMin', 'freqMax', 'windowLength', 'dynamicRange', 'pitchMin', 'pitchMax'].forEach(id => {
            const element = document.getElementById(id);
            const valueElement = document.getElementById(id + 'Value');

            element.addEventListener('input', function() {
                valueElement.textContent = this.value;
                if (audioBuffer) {
                    analyzeAndDraw();
                }
            });
        });

        // Handle select changes
        ['spectrogramType', 'colorMap'].forEach(id => {
            document.getElementById(id).addEventListener('change', function() {
                if (audioBuffer) {
                    analyzeAndDraw();
                }
            });
        });

        // Handle display option changes
        ['showWaveform', 'showSpectrogram', 'showPitch', 'showFormants', 'showIntensity'].forEach(id => {
            document.getElementById(id).addEventListener('change', function() {
                if (audioBuffer) {
                    analyzeAndDraw();
                }
            });
        });

        // Handle window resize
        window.addEventListener('resize', () => {
            if (audioBuffer) {
                analyzeAndDraw();
            }
        });

        // Initialize
        window.addEventListener('load', () => {
            resizeCanvases();
            drawGrid(waveformCtx, waveformCanvas.width, waveformCanvas.height);
            drawGrid(spectrogramCtx, spectrogramCanvas.width, spectrogramCanvas.height);
            drawGrid(pitchCtx, pitchCanvas.width, pitchCanvas.height);
            initializeSpeechRecognition();
        });

        // Web Speech API Functions
        function initializeSpeechRecognition() {
            const speechBtn = document.getElementById('speechBtn');

            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();

                // Configure speech recognition
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = document.getElementById('recognitionLanguage').value;
                recognition.maxAlternatives = 1;

                recognition.onstart = function() {
                    isRecognizing = true;
                    recognitionStartTime = Date.now();
                    document.getElementById('statusMessage').textContent = 'Speech recognition in progress...';
                };

                recognition.onresult = function(event) {
                    let finalTranscript = '';
                    let interimTranscript = '';

                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interimTranscript += transcript;
                        }
                    }

                    if (finalTranscript) {
                        // Calculate timestamp based on recognition time
                        const currentTime = (Date.now() - recognitionStartTime) / 1000;
                        const timestamp = Math.min(currentTime, audioBuffer ? audioBuffer.duration : currentTime);
                        addSpeechLabel(finalTranscript, timestamp);

                        // Redraw canvases with labels
                        if (audioBuffer) {
                            analyzeAndDraw();
                        }
                    }

                    // Display interim results in status bar
                    if (interimTranscript) {
                        document.getElementById('statusMessage').textContent = 'Recognizing: ' + interimTranscript;
                    }
                };

                recognition.onerror = function(event) {
                    console.error('Speech recognition error:', event.error);
                    document.getElementById('statusMessage').textContent = 'Error: ' + event.error;
                    stopFileRecognition();
                };

                recognition.onend = function() {
                    isRecognizing = false;
                    document.getElementById('speechBtn').textContent = 'üìù File Speech Recognition';
                    document.getElementById('speechBtn').style.background = '';
                    document.getElementById('statusMessage').textContent = 'Recognition completed';
                };
                speechBtn.disabled = false;
                document.getElementById('statusMessage').textContent = 'Speech recognition ready';
            } else {
                console.warn('Speech Recognition API not supported');
                recognition = null;
                speechBtn.disabled = false;
                document.getElementById('statusMessage').textContent = 'File speech recognition via Whisper is available';
            }
        }

        async function startFileRecognition() {
            if (!audioBuffer) {
                alert('Please upload an audio file first.');
                return;
            }

            if (isRecognizing) {
                stopFileRecognition();
                return;
            }

            if (!await getAudioFileForWhisper()) {
                alert('Failed to prepare audio file. Please try another file.');
                return;
            }

            speechLabels = [];
            const speechBtn = document.getElementById('speechBtn');

            recognitionMode = 'whisper';
            isRecognizing = true;
            whisperAbortRequested = false;
            const currentTaskId = ++whisperTaskId;

            speechBtn.textContent = '‚èπÔ∏è Stop Recognition';
            speechBtn.style.background = '#e53e3e';
            document.getElementById('statusMessage').textContent = 'Initializing Whisper (WASM)...';

            try {
                const transcriber = await getWhisperPipeline();
                if (!isActiveWhisperTask(currentTaskId)) {
                    return;
                }

                const whisperInput = await prepareWhisperInput(audioBuffer);
                if (!isActiveWhisperTask(currentTaskId)) {
                    return;
                }

                document.getElementById('statusMessage').textContent = 'Recognizing with Whisper (WASM)...';

                const languageCode = mapLanguageCode(document.getElementById('recognitionLanguage').value);
                const options = {
                    return_timestamps: true,
                    chunk_length_s: 30,
                    stride_length_s: 5,
                    task: 'transcribe',
                    sampling_rate: whisperInput.sampleRate
                };
                if (languageCode) {
                    options.language = languageCode;
                }

                const result = await transcriber(whisperInput.audio, options);

                if (!isActiveWhisperTask(currentTaskId)) {
                    return;
                }

                let labelsAdded = false;

                if (result && Array.isArray(result.chunks) && result.chunks.length > 0) {
                    result.chunks.forEach(chunk => {
                        if (chunk && chunk.text) {
                            const timestamps = chunk.timestamp || chunk.timestamps;
                            let start = 0;
                            if (Array.isArray(timestamps)) {
                                start = parseFloat(timestamps[0]) || 0;
                            } else if (timestamps && typeof timestamps.start === 'number') {
                                start = timestamps.start;
                            }
                            addSpeechLabel(chunk.text.trim(), start);
                            labelsAdded = true;
                        }
                    });
                } else if (result && typeof result.text === 'string' && result.text.trim()) {
                    addSpeechLabel(result.text.trim(), 0);
                    labelsAdded = true;
                }

                if (audioBuffer && labelsAdded) {
                    analyzeAndDraw();
                }

                console.log('Whisper recognition result (raw):', result);

                document.getElementById('statusMessage').textContent = labelsAdded ? 'Whisper recognition completed' : 'Whisper recognition completed (no text)';
            } catch (error) {
                if (whisperAbortRequested) {
                    document.getElementById('statusMessage').textContent = 'Whisper recognition interrupted';
                } else {
                    console.error('Whisper WASM transcription failed:', error);
                    document.getElementById('statusMessage').textContent = 'Whisper recognition error: ' + (error && error.message ? error.message : error);
                }
            } finally {
                if (currentTaskId === whisperTaskId) {
                    recognitionMode = null;
                    isRecognizing = false;
                    speechBtn.textContent = 'üìù File Speech Recognition';
                    speechBtn.style.background = '';
                }
            }
        }

        async function getAudioFileForWhisper() {
            if (lastAudioFile) {
                return lastAudioFile;
            }

            if (!audioBuffer) {
                return null;
            }

            try {
                const wavBuffer = audioBufferToWav(audioBuffer);
                lastAudioFile = new File([wavBuffer], 'analysis.wav', { type: 'audio/wav' });
                return lastAudioFile;
            } catch (error) {
                console.error('Failed to convert AudioBuffer to WAV:', error);
                return null;
            }
        }

        async function getWhisperPipeline() {
            if (!window.transformers || !window.transformers.pipeline) {
                throw new Error('transformers.js is not loaded');
            }

            if (!whisperPipelinePromise) {
                const { pipeline, env } = window.transformers;
                if (env.backends && env.backends.onnx && env.backends.onnx.wasm) {
                    const cores = navigator.hardwareConcurrency || 2;
                    env.backends.onnx.wasm.numThreads = Math.min(4, Math.max(1, cores - 1));
                }
                whisperPipelinePromise = pipeline('automatic-speech-recognition', WHISPER_MODEL_ID, {
                    quantized: true
                });
            }

            return whisperPipelinePromise;
        }

        async function prepareWhisperInput(buffer) {
            if (!buffer) {
                throw new Error('audio buffer is not available for Whisper');
            }

            const targetSampleRate = 16000;
            const monoData = mixAudioBufferToMono(buffer);
            const audio = buffer.sampleRate === targetSampleRate
                ? monoData
                : resampleFloat32Array(monoData, buffer.sampleRate, targetSampleRate);

            return {
                audio,
                sampleRate: targetSampleRate
            };
        }

        function mixAudioBufferToMono(buffer) {
            const channelCount = buffer.numberOfChannels;
            const length = buffer.length;

            if (channelCount === 1) {
                return new Float32Array(buffer.getChannelData(0));
            }

            const output = new Float32Array(length);
            for (let channel = 0; channel < channelCount; channel++) {
                const channelData = buffer.getChannelData(channel);
                for (let i = 0; i < length; i++) {
                    output[i] += channelData[i] / channelCount;
                }
            }
            return output;
        }

        function resampleFloat32Array(data, sourceRate, targetRate) {
            if (sourceRate === targetRate) {
                return new Float32Array(data);
            }

            const ratio = sourceRate / targetRate;
            const newLength = Math.max(1, Math.round(data.length / ratio));
            const output = new Float32Array(newLength);

            for (let i = 0; i < newLength; i++) {
                const sourceIndex = i * ratio;
                const indexLower = Math.floor(sourceIndex);
                const indexUpper = Math.min(indexLower + 1, data.length - 1);
                const weightUpper = sourceIndex - indexLower;
                const weightLower = 1 - weightUpper;
                output[i] = data[indexLower] * weightLower + data[indexUpper] * weightUpper;
            }

            return output;
        }

        function mapLanguageCode(code) {
            if (!code) return undefined;
            const lowered = code.toLowerCase();
            if (lowered === 'auto') return undefined;
            return lowered.split('-')[0];
        }

        function isActiveWhisperTask(taskId) {
            return !whisperAbortRequested && recognitionMode === 'whisper' && taskId === whisperTaskId;
        }

        function audioBufferToWav(buffer) {
            const numOfChan = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const samples = buffer.length;
            const bytesPerSample = 2;
            const blockAlign = numOfChan * bytesPerSample;
            const byteRate = sampleRate * blockAlign;
            const dataSize = samples * blockAlign;
            const bufferLength = 44 + dataSize;
            const arrayBuffer = new ArrayBuffer(bufferLength);
            const view = new DataView(arrayBuffer);

            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + dataSize, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, numOfChan, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, byteRate, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bytesPerSample * 8, true);
            writeString(view, 36, 'data');
            view.setUint32(40, dataSize, true);

            let offset = 44;
            for (let sampleIndex = 0; sampleIndex < samples; sampleIndex++) {
                for (let channel = 0; channel < numOfChan; channel++) {
                    const sample = buffer.getChannelData(channel)[sampleIndex];
                    const clamped = Math.max(-1, Math.min(1, sample));
                    const intSample = clamped < 0 ? clamped * 0x8000 : clamped * 0x7FFF;
                    view.setInt16(offset, intSample, true);
                    offset += 2;
                }
            }

            return arrayBuffer;
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        function stopFileRecognition() {
            const speechBtn = document.getElementById('speechBtn');

            if (recognitionMode === 'whisper') {
                whisperAbortRequested = true;
                recognitionMode = null;
                isRecognizing = false;
                whisperTaskId++;
                document.getElementById('statusMessage').textContent = 'Whisper recognition interrupted';
                speechBtn.textContent = 'üìù „Éï„Ç°„Ç§„É´Èü≥Â£∞Ë™çË≠ò';
                speechBtn.style.background = '';
                return;
            }

            if (recognition && isRecognizing) {
                recognition.stop();
            }
            if (audioSource && isPlaying) {
                audioSource.stop();
                isPlaying = false;
            }

            speechBtn.textContent = 'üìù „Éï„Ç°„Ç§„É´Èü≥Â£∞Ë™çË≠ò';
            speechBtn.style.background = '';
        }

        function addSpeechLabel(text, timestamp) {
            const duration = audioBuffer ? audioBuffer.duration : null;
            const normalizedTime = duration ? Math.min(Math.max(timestamp / duration, 0), 1) : 0;
            const row = speechLabels.length % 3;

            speechLabels.push({
                text: text,
                timestamp: timestamp,
                x: normalizedTime,
                row: row,
                id: Date.now()
            });

            if (speechLabels.length > 10) {
                speechLabels = speechLabels.slice(-10);
            }
        }

        function clearLabels() {
            speechLabels = [];
            updateSpeechCaptions();
            if (audioBuffer) {
                analyzeAndDraw();
            }
            document.getElementById('statusMessage').textContent = 'Labels cleared';
        }

        function updateSpeechCaptions() {
            const captionsElement = document.getElementById('speechCaptions');

            if (speechLabels.length === 0) {
                captionsElement.style.display = 'none';
                captionsElement.textContent = '';
                return;
            }

            // Sort labels by timestamp
            const sortedLabels = speechLabels.slice().sort((a, b) => a.timestamp - b.timestamp);

            // Create caption text with timestamps
            const captionTexts = sortedLabels.map(label => {
                const timeStr = label.timestamp ? `[${label.timestamp.toFixed(1)}s]` : '';
                return `${timeStr} ${label.text}`;
            });

            // Update the paragraph element
            captionsElement.style.display = 'block';
            captionsElement.textContent = captionTexts.join(' ');
        }

        // Recording Functions
        async function toggleRecording() {
            if (isRecording) {
                stopRecording();
            } else {
                await startRecording();
            }
        }

        async function startRecording() {
            try {
                // Request microphone access
                recordingStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                // Create MediaRecorder
                const mimeType = MediaRecorder.isTypeSupported('audio/webm') ? 'audio/webm' : 'audio/ogg';
                mediaRecorder = new MediaRecorder(recordingStream, {
                    mimeType: mimeType,
                    audioBitsPerSecond: 128000
                });

                recordedChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        recordedChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    // Stop speech recognition
                    if (recognition && isRecognizing) {
                        recognition.stop();
                    }

                    const extension = mimeType.includes('webm') ? 'webm' : 'ogg';
                    lastAudioFile = new File(recordedChunks, `recording.${extension}`, { type: mimeType });

                    // Convert blob to AudioBuffer
                    const arrayBuffer = await lastAudioFile.arrayBuffer();
                    initAudioContext();

                    try {
                        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                        // Update UI
                        document.getElementById('duration').textContent = audioBuffer.duration.toFixed(3);
                        document.getElementById('sampleRate').textContent = audioBuffer.sampleRate;
                        document.getElementById('statusMessage').textContent = 'Recording completed - Analyzing...';

                        // Analyze and draw
                        await analyzeAndDraw();

                        document.getElementById('statusMessage').textContent = 'Recording and analysis completed';
                    } catch (error) {
                        console.error('Error processing recorded audio:', error);
                        document.getElementById('statusMessage').textContent = 'Error: Failed to process recording';
                    }
                };

                // Start recording
                mediaRecorder.start(100); // Collect data every 100ms
                isRecording = true;
                recordingStartTime = Date.now();

                // Update UI
                document.getElementById('recordBtn').textContent = '‚èπÔ∏è Stop Recording';
                document.getElementById('recordBtn').style.background = '#e53e3e';
                document.getElementById('statusMessage').textContent = 'Recording...';

                // Start real-time speech recognition if available
                if (recognition) {
                    recognition.lang = document.getElementById('recognitionLanguage').value;
                    recognitionStartTime = Date.now();
                    setTimeout(() => {
                        recognition.start();
                    }, 100);
                }

                // Show recording time
                updateRecordingTime();

            } catch (error) {
                console.error('Error starting recording:', error);
                alert('Microphone access was denied. Please check your browser settings.');
            }
        }

        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;

                // Stop all tracks
                if (recordingStream) {
                    recordingStream.getTracks().forEach(track => track.stop());
                    recordingStream = null;
                }

                // Update UI
                document.getElementById('recordBtn').textContent = 'üéôÔ∏è Start Recording';
                document.getElementById('recordBtn').style.background = '';
                document.getElementById('statusMessage').textContent = 'Processing recording...';
            }
        }

        function updateRecordingTime() {
            if (isRecording) {
                const elapsed = ((Date.now() - recordingStartTime) / 1000).toFixed(1);
                document.getElementById('statusMessage').textContent = `Recording... ${elapsed}s`;

                // Redraw to show pulsing indicator
                if (document.getElementById('showWaveform').checked && waveformCanvas) {
                    drawRecordingIndicator(waveformCtx, waveformCanvas.width, waveformCanvas.height);
                }

                requestAnimationFrame(updateRecordingTime);
            }
        }

        // Add visual recording indicator
        function drawRecordingIndicator(ctx, width, height) {
            if (isRecording) {
                // Draw pulsing red circle
                const time = Date.now() / 1000;
                const pulse = Math.sin(time * 4) * 0.3 + 0.7;

                ctx.save();
                ctx.fillStyle = `rgba(239, 68, 68, ${pulse})`;
                ctx.beginPath();
                ctx.arc(width - 30, 30, 10, 0, 2 * Math.PI);
                ctx.fill();

                ctx.fillStyle = '#ef4444';
                ctx.font = 'bold 12px Arial';
                ctx.textAlign = 'right';
                ctx.fillText('REC', width - 45, 35);
                ctx.restore();
            }
        }

        document.querySelector('.hamburger-menu').addEventListener('click', () => {
            document.querySelector('.toolbar-buttons').classList.toggle('active');
        });

        document.getElementById('settings-toggle').addEventListener('click', () => {
            document.getElementById('sidebar-container').classList.toggle('hidden');
            // We need to resize the canvases when the sidebar is toggled
            // to ensure they fill the new available space correctly.
            setTimeout(resizeCanvases, 300); // Wait for the transition to finish
        });
    </script>
</body>
</html>
